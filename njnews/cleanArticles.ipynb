{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.9 64-bit ('holcombEnv': conda)"
  },
  "interpreter": {
   "hash": "044a0165a1a561273ca818f13158b4edea630051cf64f3770cf4dedc4e818f34"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from datetime import datetime as dt\n",
    "from bs4 import BeautifulSoup, element\n",
    "import time as tm\n",
    "import os\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars_to_remove = \",^\\()|\\\"\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function constructs the url for an archived webpage according to https://wayback.archive-it.org formatting\n",
    "def get_html(collection_id, folder, data):\n",
    "\n",
    "    # creates a local directory named 'folder' if it doesn't exist\n",
    "    if not os.path.isdir(folder):\n",
    "        os.mkdir(folder)\n",
    "\n",
    "    # the base url for the archive\n",
    "    ai_url = \"https://wayback.archive-it.org\"\n",
    "\n",
    "    # this gets a unique list of dates and urls so we can avoid duplication\n",
    "    date_list = pd.to_datetime(data['date'].unique())\n",
    "    urls = data['url'].unique()\n",
    "\n",
    "    # this loop iterates through all the urls, searches for the archived webpage\n",
    "    # then looks for any hyperlinks on the page that are associated with a date in the list\n",
    "    for url in urls:\n",
    "\n",
    "        # this is the url of an archived webpage's home \n",
    "        # (e.g. https://wayback.archive-it.org/12706/*/http://mypaperonline.com)\n",
    "        home = ai_url + '/' + str(collection_id) + '/*/' + url\n",
    "\n",
    "        # this block gets all the date hyperlinks\n",
    "        page = requests.get(home)\n",
    "        soup = BeautifulSoup(page.content, \"lxml\")\n",
    "        alist = soup.find_all(\"a\")\n",
    "        alist = [a for a in alist if 'onclick' in a.attrs]\n",
    "        dates = [dt.strptime(a.text, '%b %d, %Y') for a in alist]\n",
    "\n",
    "        # this loops through all the date hyperlinks and if they're in\n",
    "        # the date_list it saves them locally\n",
    "        for a in alist:\n",
    "\n",
    "            # this creates a local path for the html, it converts URLS+dates\n",
    "            # into unique simple strings\n",
    "            path = folder + '/' + dt.strptime(a.text, '%b %d, %Y').strftime('%Y%m%d') + '_' + ''.join(re.findall('(\\d+|[a-zA-Z]+|-|\\.)', url)) + '.html'\n",
    "\n",
    "            # if url == 'http://www.newarkblack.com/queens-new-york-annual-pop-shop-artist-market-set-held-multiple-dates-nov-25-2017-december-23-22017':\n",
    "            #     print('path')\n",
    "            #     print('https:' + a.attrs['href'])\n",
    "            #     print(dt.strptime(a.text, '%b %d, %Y') in date_list)\n",
    "            #     print(not os.path.isfile(path))\n",
    "\n",
    "            # this is where it saves the html ('onclick' in a.attrs is probably redundant)\n",
    "            if dt.strptime(a.text, '%b %d, %Y') in date_list and not os.path.isfile(path):\n",
    "                wp = requests.get('https:' + a.attrs['href'])\n",
    "                f = open(path, 'wb')\n",
    "                f.write(wp.content)\n",
    "                f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "chosen = pd.read_csv('final_chosen.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "completed in  929.747517824173  seconds\n"
     ]
    }
   ],
   "source": [
    "start = tm.time()\n",
    "get_html(12706, 'chosenSamp', chosen)\n",
    "span = tm.time() - start\n",
    "print('completed in ', span, ' seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "completed in  377.32148933410645  seconds\n"
     ]
    }
   ],
   "source": [
    "ctr = 0\n",
    "folder = 'chosenSamp'\n",
    "\n",
    "# this creates and sets all the new variables to None\n",
    "chosen['cleaned'] = None\n",
    "chosen['title'] = None\n",
    "chosen['published'] = None\n",
    "chosen['author'] = None\n",
    "\n",
    "start = tm.time()\n",
    "\n",
    "for index, row in chosen.iterrows():\n",
    "\n",
    "    # these arrays are only expected to have len()==1 but may have more\n",
    "    # if there exist multiple objects\n",
    "    articles = []\n",
    "    titles = []\n",
    "    published = []\n",
    "    author = []\n",
    "\n",
    "    # this constructs the path of the html file\n",
    "    path = folder + '/' + ''.join(re.findall('\\d+', row.date)) + '_' + ''.join(re.findall('(\\d+|[a-zA-Z]+|-|\\.)', row.url)) + '.html'\n",
    "\n",
    "    # reads the html\n",
    "    f = open(path, 'rb')\n",
    "    soup = BeautifulSoup(f.read(), \"lxml\")\n",
    "    \n",
    "    # collects variables from Morristown Life webpages\n",
    "    if row['domain'] == 'www.mypaperonline.com':\n",
    "        articles = soup.find_all(class_ = \"entry-content\")\n",
    "        titles = soup.find_all(class_ = \"entry-title single-post-title\")\n",
    "        published = soup.find_all(attrs = {'itemprop' : 'datePublished'})\n",
    "        author = soup.find_all(class_ = \"author vcard\")\n",
    "\n",
    "    # collects variables from New Brunswick Today webpages\n",
    "    if row['domain'] in ['newbrunswicktoday.com', 'www.newbrunswicktoday.com'] :\n",
    "        articles = soup.find_all(attrs = {'property' : 'dc:description'})\n",
    "        titles = soup.find_all(attrs = {'property' : 'dc:title'})\n",
    "        published = soup.find_all(attrs = {'property' : 'dc:issued'})\n",
    "        author = soup.find_all(attrs = {'property' : 'dc:creator'})\n",
    "\n",
    "    # collects variables from Newark Black\n",
    "    if row['domain'] == 'www.newarkblack.com':\n",
    "        articles = soup.find_all(class_ = 'td-post-content td-pb-padding-side')\n",
    "        titles = soup.find_all('h1', class_ = 'entry-title')\n",
    "        published = soup.find_all(attrs = {'itemprop' : 'dateCreated'})\n",
    "        author = soup.find_all(attrs = {'itemprop' : 'author'})\n",
    "\n",
    "    # overwrites the text variable with the cleaner extracted version\n",
    "    if len(articles) == 1: \n",
    "        ctr = ctr + 1\n",
    "        text = articles[0].text.translate ({ord(c): \"\" for c in chars_to_remove})\n",
    "        chosen.loc[index, 'text'] = text\n",
    "        chosen.loc[index, 'cleaned'] = 1\n",
    "    else:\n",
    "        chosen.loc[index, 'cleaned'] = 0\n",
    "\n",
    "    # adds the title variable\n",
    "    if len(titles) >= 1:\n",
    "        chosen.loc[index, 'title'] = titles[0].text.translate ({ord(c): \"\" for c in chars_to_remove})\n",
    "    \n",
    "    # adds the published variable\n",
    "    if len(published) >= 1:\n",
    "        chosen.loc[index, 'published'] = published[0].text.translate ({ord(c): \"\" for c in chars_to_remove})\n",
    "\n",
    "    # adds the author variable\n",
    "    if len(author) >= 1:\n",
    "        chosen.loc[index, 'author'] = author[0].text.translate ({ord(c): \"\" for c in chars_to_remove})\n",
    "\n",
    "    # these if statements are all warnings if there are duplicative extractions\n",
    "    # if len(articles) > 1:\n",
    "    #     print(row.url, \" has multiple article objects\")\n",
    "    \n",
    "    # if len(titles) > 1:\n",
    "    #     print(row.url, \" has multiple titles objects\")\n",
    "\n",
    "    # if len(published) > 1:\n",
    "    #     print(row.url, \" has multiple published objects\")\n",
    "\n",
    "    # if len(author) > 1:\n",
    "    #     print(row.url, \" has multiple author objects\")\n",
    "\n",
    "    f.close()\n",
    "\n",
    "    tm.sleep(.01)\n",
    "\n",
    "span = tm.time() - start\n",
    "\n",
    "print('completed in ', span, ' seconds')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1350\n"
     ]
    }
   ],
   "source": [
    "print(ctr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "      Unnamed: 0        date                 domain  \\\n",
       "0             16  2019-10-08    www.newarkblack.com   \n",
       "1             17  2019-10-08    www.newarkblack.com   \n",
       "2             18  2019-10-08    www.newarkblack.com   \n",
       "3             19  2019-10-08    www.newarkblack.com   \n",
       "4             20  2019-10-08    www.newarkblack.com   \n",
       "...          ...         ...                    ...   \n",
       "2408        4553  2019-11-02  www.mypaperonline.com   \n",
       "2409        4554  2019-11-02  www.mypaperonline.com   \n",
       "2410        4557  2019-11-02  www.mypaperonline.com   \n",
       "2411        4565  2019-11-02  www.mypaperonline.com   \n",
       "2412        4750  2019-10-12  www.mypaperonline.com   \n",
       "\n",
       "                                                    url  \\\n",
       "0                            http://www.newarkblack.com   \n",
       "1                            http://www.newarkblack.com   \n",
       "2     http://www.newarkblack.com/?s={search_term_str...   \n",
       "3                  http://www.newarkblack.com/advertise   \n",
       "4              http://www.newarkblack.com/category/news   \n",
       "...                                                 ...   \n",
       "2408  https://www.mypaperonline.com/first-library-wi...   \n",
       "2409  https://www.mypaperonline.com/hackettstowns-he...   \n",
       "2410  https://www.mypaperonline.com/dont-miss-these-...   \n",
       "2411  http://www.mypaperonline.com/10th-anniversary-...   \n",
       "2412  https://www.mypaperonline.com/hauntings-around...   \n",
       "\n",
       "                                                   text cleaned  \\\n",
       "0     NewarkBlack.com - Newark African American News...       0   \n",
       "1                                                    �\b       0   \n",
       "2     You searched for {search_term_string} - Newark...       0   \n",
       "3     Advertise - NewarkBlack.com CLOSE Home Adverti...       0   \n",
       "4     News Archives - NewarkBlack.com CLOSE Home Adv...       0   \n",
       "...                                                 ...     ...   \n",
       "2408  By: Michele DiPasquale To some a library may b...       1   \n",
       "2409  By Dawn M Chiossi  Going on since February of ...       1   \n",
       "2410  BPT – Snacks produce drinks or sweets – if it ...       1   \n",
       "2411  10th Anniversary Golf Classic Scores High For ...       1   \n",
       "2412  by Elsie Walker Halloween will soon be here.  ...       1   \n",
       "\n",
       "                                                  title          published  \\\n",
       "0                                                  None  September 25 2019   \n",
       "1                                                  None  September 25 2019   \n",
       "2                {search_term_string} -  search results     October 7 2019   \n",
       "3                                                  None     October 7 2019   \n",
       "4                                                  News     October 3 2019   \n",
       "...                                                 ...                ...   \n",
       "2408  First Library Will Help Build Libraries of Babies     October 1 2019   \n",
       "2409  Hackettstown’s Heath Village Inspires Staff wi...     October 1 2019   \n",
       "2410  Don’t miss these must-have snacks that define ...       April 3 2016   \n",
       "2411  10th Anniversary Golf Classic Scores High For ...      August 9 2015   \n",
       "2412            Hauntings Around the Area for Halloween     October 1 2019   \n",
       "\n",
       "                           author  \n",
       "0                                  \n",
       "1                                  \n",
       "2                                  \n",
       "3                                  \n",
       "4     WA Public Relations Company  \n",
       "...                           ...  \n",
       "2408               new_view_media  \n",
       "2409               new_view_media  \n",
       "2410               new_view_media  \n",
       "2411               new_view_media  \n",
       "2412               new_view_media  \n",
       "\n",
       "[2413 rows x 9 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unnamed: 0</th>\n      <th>date</th>\n      <th>domain</th>\n      <th>url</th>\n      <th>text</th>\n      <th>cleaned</th>\n      <th>title</th>\n      <th>published</th>\n      <th>author</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>16</td>\n      <td>2019-10-08</td>\n      <td>www.newarkblack.com</td>\n      <td>http://www.newarkblack.com</td>\n      <td>NewarkBlack.com - Newark African American News...</td>\n      <td>0</td>\n      <td>None</td>\n      <td>September 25 2019</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>17</td>\n      <td>2019-10-08</td>\n      <td>www.newarkblack.com</td>\n      <td>http://www.newarkblack.com</td>\n      <td>�\b</td>\n      <td>0</td>\n      <td>None</td>\n      <td>September 25 2019</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>18</td>\n      <td>2019-10-08</td>\n      <td>www.newarkblack.com</td>\n      <td>http://www.newarkblack.com/?s={search_term_str...</td>\n      <td>You searched for {search_term_string} - Newark...</td>\n      <td>0</td>\n      <td>{search_term_string} -  search results</td>\n      <td>October 7 2019</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>19</td>\n      <td>2019-10-08</td>\n      <td>www.newarkblack.com</td>\n      <td>http://www.newarkblack.com/advertise</td>\n      <td>Advertise - NewarkBlack.com CLOSE Home Adverti...</td>\n      <td>0</td>\n      <td>None</td>\n      <td>October 7 2019</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>20</td>\n      <td>2019-10-08</td>\n      <td>www.newarkblack.com</td>\n      <td>http://www.newarkblack.com/category/news</td>\n      <td>News Archives - NewarkBlack.com CLOSE Home Adv...</td>\n      <td>0</td>\n      <td>News</td>\n      <td>October 3 2019</td>\n      <td>WA Public Relations Company</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>2408</th>\n      <td>4553</td>\n      <td>2019-11-02</td>\n      <td>www.mypaperonline.com</td>\n      <td>https://www.mypaperonline.com/first-library-wi...</td>\n      <td>By: Michele DiPasquale To some a library may b...</td>\n      <td>1</td>\n      <td>First Library Will Help Build Libraries of Babies</td>\n      <td>October 1 2019</td>\n      <td>new_view_media</td>\n    </tr>\n    <tr>\n      <th>2409</th>\n      <td>4554</td>\n      <td>2019-11-02</td>\n      <td>www.mypaperonline.com</td>\n      <td>https://www.mypaperonline.com/hackettstowns-he...</td>\n      <td>By Dawn M Chiossi  Going on since February of ...</td>\n      <td>1</td>\n      <td>Hackettstown’s Heath Village Inspires Staff wi...</td>\n      <td>October 1 2019</td>\n      <td>new_view_media</td>\n    </tr>\n    <tr>\n      <th>2410</th>\n      <td>4557</td>\n      <td>2019-11-02</td>\n      <td>www.mypaperonline.com</td>\n      <td>https://www.mypaperonline.com/dont-miss-these-...</td>\n      <td>BPT – Snacks produce drinks or sweets – if it ...</td>\n      <td>1</td>\n      <td>Don’t miss these must-have snacks that define ...</td>\n      <td>April 3 2016</td>\n      <td>new_view_media</td>\n    </tr>\n    <tr>\n      <th>2411</th>\n      <td>4565</td>\n      <td>2019-11-02</td>\n      <td>www.mypaperonline.com</td>\n      <td>http://www.mypaperonline.com/10th-anniversary-...</td>\n      <td>10th Anniversary Golf Classic Scores High For ...</td>\n      <td>1</td>\n      <td>10th Anniversary Golf Classic Scores High For ...</td>\n      <td>August 9 2015</td>\n      <td>new_view_media</td>\n    </tr>\n    <tr>\n      <th>2412</th>\n      <td>4750</td>\n      <td>2019-10-12</td>\n      <td>www.mypaperonline.com</td>\n      <td>https://www.mypaperonline.com/hauntings-around...</td>\n      <td>by Elsie Walker Halloween will soon be here.  ...</td>\n      <td>1</td>\n      <td>Hauntings Around the Area for Halloween</td>\n      <td>October 1 2019</td>\n      <td>new_view_media</td>\n    </tr>\n  </tbody>\n</table>\n<p>2413 rows × 9 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "chosen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "chosen.drop('Unnamed: 0', axis=1).to_csv('cleaned1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}