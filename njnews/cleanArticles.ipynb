{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python379jvsc74a57bd0044a0165a1a561273ca818f13158b4edea630051cf64f3770cf4dedc4e818f34",
   "display_name": "Python 3.7.9 64-bit ('holcombEnv': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from datetime import datetime as dt\n",
    "from bs4 import BeautifulSoup, element\n",
    "import time as tm\n",
    "import os\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars_to_remove = \",^\\()|\\\"\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function constructs the url for an archived webpage according to https://wayback.archive-it.org formatting\n",
    "def get_html(collection_id, folder, data):\n",
    "\n",
    "    # creates a local directory named 'folder' if it doesn't exist\n",
    "    if not os.path.isdir(folder):\n",
    "        os.mkdir(folder)\n",
    "\n",
    "    # the base url for the archive\n",
    "    ai_url = \"https://wayback.archive-it.org\"\n",
    "\n",
    "    # this gets a unique list of dates and urls so we can avoid duplication\n",
    "    date_list = pd.to_datetime(data['date'].unique())\n",
    "    urls = data['url'].unique()\n",
    "\n",
    "    # this loop iterates through all the urls, searches for the archived webpage\n",
    "    # then looks for any hyperlinks on the page that are associated with a date in the list\n",
    "    for url in urls:\n",
    "\n",
    "        # this is the url of an archived webpage's home \n",
    "        # (e.g. https://wayback.archive-it.org/12706/*/http://mypaperonline.com)\n",
    "        home = ai_url + '/' + str(collection_id) + '/*/' + url\n",
    "\n",
    "        # this block gets all the date hyperlinks\n",
    "        page = requests.get(home)\n",
    "        soup = BeautifulSoup(page.content, \"lxml\")\n",
    "        alist = soup.find_all(\"a\")\n",
    "        alist = [a for a in alist if 'onclick' in a.attrs]\n",
    "        dates = [dt.strptime(a.text, '%b %d, %Y') for a in alist]\n",
    "\n",
    "        # this loops through all the date hyperlinks and if they're in\n",
    "        # the date_list it saves them locally\n",
    "        for a in alist:\n",
    "\n",
    "            # this creates a local path for the html, it converts URLS+dates\n",
    "            # into unique simple strings\n",
    "            path = folder + '/' + dt.strptime(a.text, '%b %d, %Y').strftime('%Y%m%d') + '_' + ''.join(re.findall('(\\d+|[a-zA-Z]+|-|\\.)', url)) + '.html'\n",
    "\n",
    "            if url == 'http://www.newarkblack.com/queens-new-york-annual-pop-shop-artist-market-set-held-multiple-dates-nov-25-2017-december-23-22017':\n",
    "                print('path')\n",
    "                print('https:' + a.attrs['href'])\n",
    "                print(dt.strptime(a.text, '%b %d, %Y') in date_list)\n",
    "                print(not os.path.isfile(path))\n",
    "\n",
    "            # this is where it saves the html ('onclick' in a.attrs is probably redundant)\n",
    "            if dt.strptime(a.text, '%b %d, %Y') in date_list and not os.path.isfile(path):\n",
    "                wp = requests.get('https:' + a.attrs['href'])\n",
    "                f = open(path, 'wb')\n",
    "                f.write(wp.content)\n",
    "                f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "chosen = pd.read_csv('final_chosen.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "path\nhttps://wayback.archive-it.org/12706/20191008192629/http://www.newarkblack.com/queens-new-york-annual-pop-shop-artist-market-set-held-multiple-dates-nov-25-2017-december-23-22017/\nTrue\nFalse\npath\nhttps://wayback.archive-it.org/12706/20191012181003/http://www.newarkblack.com/queens-new-york-annual-pop-shop-artist-market-set-held-multiple-dates-nov-25-2017-december-23-22017/\nTrue\nFalse\npath\nhttps://wayback.archive-it.org/12706/20191020191017/http://www.newarkblack.com/queens-new-york-annual-pop-shop-artist-market-set-held-multiple-dates-nov-25-2017-december-23-22017/\nTrue\nFalse\npath\nhttps://wayback.archive-it.org/12706/20191024201121/http://www.newarkblack.com/queens-new-york-annual-pop-shop-artist-market-set-held-multiple-dates-nov-25-2017-december-23-22017/\nTrue\nFalse\npath\nhttps://wayback.archive-it.org/12706/20191030190212/http://www.newarkblack.com/queens-new-york-annual-pop-shop-artist-market-set-held-multiple-dates-nov-25-2017-december-23-22017/\nTrue\nFalse\npath\nhttps://wayback.archive-it.org/12706/20191102020043/http://www.newarkblack.com/queens-new-york-annual-pop-shop-artist-market-set-held-multiple-dates-nov-25-2017-december-23-22017/\nTrue\nFalse\npath\nhttps://wayback.archive-it.org/12706/20191111201820/http://www.newarkblack.com/queens-new-york-annual-pop-shop-artist-market-set-held-multiple-dates-nov-25-2017-december-23-22017/\nTrue\nFalse\n"
     ]
    }
   ],
   "source": [
    "get_html(12706, 'chosenSamp', chosen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-ee5e034985b0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     23\u001b[0m         \u001b[0mtitles\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msoup\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_all\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mattrs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'property'\u001b[0m \u001b[1;33m:\u001b[0m \u001b[1;34m'dc:title'\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m         \u001b[0mpublished\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msoup\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_all\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mattrs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'property'\u001b[0m \u001b[1;33m:\u001b[0m \u001b[1;34m'dc:issued'\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m         \u001b[0mauthor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msoup\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_all\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mattrs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'property'\u001b[0m \u001b[1;33m:\u001b[0m \u001b[1;34m'dc:creator'\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mrow\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'domain'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'www.newarkblack.com'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\holcombEnv\\lib\\site-packages\\bs4\\element.py\u001b[0m in \u001b[0;36mfind_all\u001b[1;34m(self, name, attrs, recursive, text, limit, **kwargs)\u001b[0m\n\u001b[0;32m   1786\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mrecursive\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1787\u001b[0m             \u001b[0mgenerator\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1788\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_find_all\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlimit\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1789\u001b[0m     \u001b[0mfindAll\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfind_all\u001b[0m       \u001b[1;31m# BS3\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1790\u001b[0m     \u001b[0mfindChildren\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfind_all\u001b[0m  \u001b[1;31m# BS2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\holcombEnv\\lib\\site-packages\\bs4\\element.py\u001b[0m in \u001b[0;36m_find_all\u001b[1;34m(self, name, attrs, text, limit, generator, **kwargs)\u001b[0m\n\u001b[0;32m    781\u001b[0m                 \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    782\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 783\u001b[1;33m                 \u001b[0mfound\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    784\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mfound\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    785\u001b[0m                     \u001b[0mresults\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfound\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\holcombEnv\\lib\\site-packages\\bs4\\element.py\u001b[0m in \u001b[0;36msearch\u001b[1;34m(self, markup)\u001b[0m\n\u001b[0;32m   2063\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmarkup\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTag\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2064\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mattrs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2065\u001b[1;33m                 \u001b[0mfound\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msearch_tag\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmarkup\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2066\u001b[0m         \u001b[1;31m# If it's text, make sure the text matches.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2067\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmarkup\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mNavigableString\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\holcombEnv\\lib\\site-packages\\bs4\\element.py\u001b[0m in \u001b[0;36msearch_tag\u001b[1;34m(self, markup_name, markup_attrs)\u001b[0m\n\u001b[0;32m   2026\u001b[0m                                 \u001b[0mmarkup_attr_map\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mv\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2027\u001b[0m                     \u001b[0mattr_value\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmarkup_attr_map\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mattr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2028\u001b[1;33m                     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_matches\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mattr_value\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmatch_against\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2029\u001b[0m                         \u001b[0mmatch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2030\u001b[0m                         \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\holcombEnv\\lib\\site-packages\\bs4\\element.py\u001b[0m in \u001b[0;36m_matches\u001b[1;34m(self, markup, match_against, already_tried)\u001b[0m\n\u001b[0;32m   2100\u001b[0m         \u001b[1;31m# other ways of matching match the tag name as a string.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2101\u001b[0m         \u001b[0moriginal_markup\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmarkup\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2102\u001b[1;33m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmarkup\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTag\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2103\u001b[0m             \u001b[0mmarkup\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmarkup\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2104\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "ctr = 0\n",
    "folder = 'chosenSamp'\n",
    "chosen['cleaned'] = None\n",
    "chosen['title'] = None\n",
    "chosen['published'] = None\n",
    "chosen['author'] = None\n",
    "\n",
    "for index, row in chosen.iterrows():\n",
    "\n",
    "    path = folder + '/' + ''.join(re.findall('\\d+', row.date)) + '_' + ''.join(re.findall('(\\d+|[a-zA-Z]+|-|\\.)', row.url)) + '.html'\n",
    "\n",
    "    f = open(path, 'rb')\n",
    "    soup = BeautifulSoup(f.read(), \"lxml\")\n",
    "    \n",
    "    if row['domain'] == 'www.mypaperonline.com':\n",
    "        articles = soup.find_all(class_ = \"entry-content\")\n",
    "        titles = soup.find_all(class_ = \"entry-title single-post-title\")\n",
    "        published = soup.find_all(attrs = {'itemprop' : 'datePublished'})\n",
    "        author = soup.find_all(class_ = \"author vcard\")\n",
    "\n",
    "    if row['domain'] in ['newbrunswicktoday.com', 'www.newbrunswicktoday.com'] :\n",
    "        articles = soup.find_all(attrs = {'property' : 'dc:description'})\n",
    "        titles = soup.find_all(attrs = {'property' : 'dc:title'})\n",
    "        published = soup.find_all(attrs = {'property' : 'dc:issued'})\n",
    "        author = soup.find_all(attrs = {'property' : 'dc:creator'})\n",
    "\n",
    "    if row['domain'] == 'www.newarkblack.com':\n",
    "        articles = soup.find_all(class_ = 'td-post-content td-pb-padding-side')\n",
    "        titles = soup.find_all(class_ = 'entry-title')\n",
    "        published = soup.find_all(attrs = {'itemprop' : 'dateCreated'})\n",
    "        author = soup.find_all(attrs = {'itemprop' : 'author'})\n",
    "\n",
    "    if len(articles) == 1: \n",
    "        ctr = ctr + 1\n",
    "        text = articles[0].text.translate ({ord(c): \"\" for c in chars_to_remove})\n",
    "        chosen.loc[index, 'text'] = text\n",
    "        chosen.loc[index, 'cleaned'] = 1\n",
    "    else:\n",
    "        chosen.loc[index, 'cleaned'] = 0\n",
    "\n",
    "    if len(titles) >= 1:\n",
    "        chosen.loc[index, 'title'] = titles[0].text.translate ({ord(c): \"\" for c in chars_to_remove})\n",
    "    \n",
    "    if len(published) >= 1:\n",
    "        chosen.loc[index, 'published'] = published[0].text.translate ({ord(c): \"\" for c in chars_to_remove})\n",
    "\n",
    "    if len(author) >= 1:\n",
    "        chosen.loc[index, 'author'] = author[0].text.translate ({ord(c): \"\" for c in chars_to_remove})\n",
    "\n",
    "\n",
    "    f.close()\n",
    "\n",
    "    tm.sleep(.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1368\n"
     ]
    }
   ],
   "source": [
    "print(ctr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "      Unnamed: 0        date                 domain  \\\n",
       "0             16  2019-10-08    www.newarkblack.com   \n",
       "1             17  2019-10-08    www.newarkblack.com   \n",
       "2             18  2019-10-08    www.newarkblack.com   \n",
       "3             19  2019-10-08    www.newarkblack.com   \n",
       "4             20  2019-10-08    www.newarkblack.com   \n",
       "...          ...         ...                    ...   \n",
       "2408        4553  2019-11-02  www.mypaperonline.com   \n",
       "2409        4554  2019-11-02  www.mypaperonline.com   \n",
       "2410        4557  2019-11-02  www.mypaperonline.com   \n",
       "2411        4565  2019-11-02  www.mypaperonline.com   \n",
       "2412        4750  2019-10-12  www.mypaperonline.com   \n",
       "\n",
       "                                                    url  \\\n",
       "0                            http://www.newarkblack.com   \n",
       "1                            http://www.newarkblack.com   \n",
       "2     http://www.newarkblack.com/?s={search_term_str...   \n",
       "3                  http://www.newarkblack.com/advertise   \n",
       "4              http://www.newarkblack.com/category/news   \n",
       "...                                                 ...   \n",
       "2408  https://www.mypaperonline.com/first-library-wi...   \n",
       "2409  https://www.mypaperonline.com/hackettstowns-he...   \n",
       "2410  https://www.mypaperonline.com/dont-miss-these-...   \n",
       "2411  http://www.mypaperonline.com/10th-anniversary-...   \n",
       "2412  https://www.mypaperonline.com/hauntings-around...   \n",
       "\n",
       "                                                   text cleaned  \n",
       "0     NewarkBlack.com - Newark African American News...       0  \n",
       "1                                                    �\b       0  \n",
       "2     You searched for {search_term_string} - Newark...       0  \n",
       "3     Advertise - NewarkBlack.com CLOSE Home Adverti...       0  \n",
       "4     News Archives - NewarkBlack.com CLOSE Home Adv...       0  \n",
       "...                                                 ...     ...  \n",
       "2408  By: Michele DiPasquale To some a library may b...       1  \n",
       "2409  By Dawn M Chiossi  Going on since February of ...       1  \n",
       "2410  BPT – Snacks produce drinks or sweets – if it ...       1  \n",
       "2411  10th Anniversary Golf Classic Scores High For ...       1  \n",
       "2412  by Elsie Walker Halloween will soon be here.  ...       1  \n",
       "\n",
       "[2413 rows x 6 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unnamed: 0</th>\n      <th>date</th>\n      <th>domain</th>\n      <th>url</th>\n      <th>text</th>\n      <th>cleaned</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>16</td>\n      <td>2019-10-08</td>\n      <td>www.newarkblack.com</td>\n      <td>http://www.newarkblack.com</td>\n      <td>NewarkBlack.com - Newark African American News...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>17</td>\n      <td>2019-10-08</td>\n      <td>www.newarkblack.com</td>\n      <td>http://www.newarkblack.com</td>\n      <td>�\b</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>18</td>\n      <td>2019-10-08</td>\n      <td>www.newarkblack.com</td>\n      <td>http://www.newarkblack.com/?s={search_term_str...</td>\n      <td>You searched for {search_term_string} - Newark...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>19</td>\n      <td>2019-10-08</td>\n      <td>www.newarkblack.com</td>\n      <td>http://www.newarkblack.com/advertise</td>\n      <td>Advertise - NewarkBlack.com CLOSE Home Adverti...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>20</td>\n      <td>2019-10-08</td>\n      <td>www.newarkblack.com</td>\n      <td>http://www.newarkblack.com/category/news</td>\n      <td>News Archives - NewarkBlack.com CLOSE Home Adv...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>2408</th>\n      <td>4553</td>\n      <td>2019-11-02</td>\n      <td>www.mypaperonline.com</td>\n      <td>https://www.mypaperonline.com/first-library-wi...</td>\n      <td>By: Michele DiPasquale To some a library may b...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2409</th>\n      <td>4554</td>\n      <td>2019-11-02</td>\n      <td>www.mypaperonline.com</td>\n      <td>https://www.mypaperonline.com/hackettstowns-he...</td>\n      <td>By Dawn M Chiossi  Going on since February of ...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2410</th>\n      <td>4557</td>\n      <td>2019-11-02</td>\n      <td>www.mypaperonline.com</td>\n      <td>https://www.mypaperonline.com/dont-miss-these-...</td>\n      <td>BPT – Snacks produce drinks or sweets – if it ...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2411</th>\n      <td>4565</td>\n      <td>2019-11-02</td>\n      <td>www.mypaperonline.com</td>\n      <td>http://www.mypaperonline.com/10th-anniversary-...</td>\n      <td>10th Anniversary Golf Classic Scores High For ...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2412</th>\n      <td>4750</td>\n      <td>2019-10-12</td>\n      <td>www.mypaperonline.com</td>\n      <td>https://www.mypaperonline.com/hauntings-around...</td>\n      <td>by Elsie Walker Halloween will soon be here.  ...</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n<p>2413 rows × 6 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "chosen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "chosen.drop('Unnamed: 0', axis=1).to_csv('cleaned1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}