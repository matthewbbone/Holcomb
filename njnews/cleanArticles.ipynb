{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python379jvsc74a57bd0044a0165a1a561273ca818f13158b4edea630051cf64f3770cf4dedc4e818f34",
   "display_name": "Python 3.7.9 64-bit ('holcombEnv': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from datetime import datetime as dt\n",
    "from bs4 import BeautifulSoup, element\n",
    "import time as tm\n",
    "import os\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars_to_remove = \",^\\()|\\\"\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function constructs the url for an archived webpage according to https://wayback.archive-it.org formatting\n",
    "def get_html(collection_id, folder, data):\n",
    "\n",
    "    # creates a local directory named 'folder' if it doesn't exist\n",
    "    if not os.path.isdir(folder):\n",
    "        os.mkdir(folder)\n",
    "\n",
    "    # the base url for the archive\n",
    "    ai_url = \"https://wayback.archive-it.org\"\n",
    "\n",
    "    # this gets a unique list of dates and urls so we can avoid duplication\n",
    "    date_list = pd.to_datetime(data['date'].unique())\n",
    "    urls = data['url'].unique()\n",
    "\n",
    "    # this loop iterates through all the urls, searches for the archived webpage\n",
    "    # then looks for any hyperlinks on the page that are associated with a date in the list\n",
    "    for url in urls:\n",
    "\n",
    "        # this is the url of an archived webpage's home \n",
    "        # (e.g. https://wayback.archive-it.org/12706/*/http://mypaperonline.com)\n",
    "        home = ai_url + '/' + str(collection_id) + '/*/' + url\n",
    "\n",
    "        # this block gets all the date hyperlinks\n",
    "        page = requests.get(home)\n",
    "        soup = BeautifulSoup(page.content, \"lxml\")\n",
    "        alist = soup.find_all(\"a\")\n",
    "        alist = [a for a in alist if 'onclick' in a.attrs]\n",
    "        dates = [dt.strptime(a.text, '%b %d, %Y') for a in alist]\n",
    "\n",
    "        # this loops through all the date hyperlinks and if they're in\n",
    "        # the date_list it saves them locally\n",
    "        for a in alist:\n",
    "\n",
    "            # this creates a local path for the html, it converts URLS+dates\n",
    "            # into unique simple strings\n",
    "            path = folder + '/' + dt.strptime(a.text, '%b %d, %Y').strftime('%Y%m%d') + '_' + ''.join(re.findall('(\\d+|[a-zA-Z]+|-|\\.)', url.split('/')[-1])) + '.html'\n",
    "\n",
    "            if url == 'http://www.newarkblack.com/homeless-black-teen-gets-accepted-into-17-colleges-and-he-was-not-part-of-a-cheating-scandal/':\n",
    "                print(path)\n",
    "                print('https:' + a.attrs['href'])\n",
    "                print(dt.strptime(a.text, '%b %d, %Y') in date_list)\n",
    "                print(not os.path.isfile(path))\n",
    "\n",
    "            # this is where it saves the html ('onclick' in a.attrs is probably redundant)\n",
    "            if dt.strptime(a.text, '%b %d, %Y') in date_list and not os.path.isfile(path):\n",
    "                wp = requests.get('https:' + a.attrs['href'])\n",
    "                f = open(path, 'wb')\n",
    "                f.write(wp.content)\n",
    "                f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "chosen = pd.read_csv('final_chosen.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "chosenSamp/20191001_townships-minimum-wage-violations-date-back-decade.html\nhttps://wayback.archive-it.org/12706/20191001141403/https://newbrunswicktoday.com/article/townships-minimum-wage-violations-date-back-decade\nTrue\nFalse\nchosenSamp/20191008_townships-minimum-wage-violations-date-back-decade.html\nhttps://wayback.archive-it.org/12706/20191008231603/https://newbrunswicktoday.com/article/townships-minimum-wage-violations-date-back-decade\nTrue\nFalse\nchosenSamp/20191012_townships-minimum-wage-violations-date-back-decade.html\nhttps://wayback.archive-it.org/12706/20191012212023/https://newbrunswicktoday.com/article/townships-minimum-wage-violations-date-back-decade\nTrue\nFalse\nchosenSamp/20191020_townships-minimum-wage-violations-date-back-decade.html\nhttps://wayback.archive-it.org/12706/20191020220952/https://newbrunswicktoday.com/article/townships-minimum-wage-violations-date-back-decade\nTrue\nFalse\nchosenSamp/20191024_townships-minimum-wage-violations-date-back-decade.html\nhttps://wayback.archive-it.org/12706/20191024230519/https://newbrunswicktoday.com/article/townships-minimum-wage-violations-date-back-decade\nTrue\nFalse\nchosenSamp/20191030_townships-minimum-wage-violations-date-back-decade.html\nhttps://wayback.archive-it.org/12706/20191030220212/https://newbrunswicktoday.com/article/townships-minimum-wage-violations-date-back-decade\nTrue\nFalse\n"
     ]
    }
   ],
   "source": [
    "get_html(12706, 'chosenSamp', chosen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ctr = 0\n",
    "folder = 'chosenSamp'\n",
    "chosen['article'] = 0\n",
    "\n",
    "for index, row in chosen.iterrows():\n",
    "\n",
    "    path = folder + '/' + ''.join(re.findall('\\d+', row.date)) + '_' + ''.join(re.findall('(\\d+|[a-zA-Z]+|-|\\.)', row.url.split('/')[-1])) + '.html'\n",
    "\n",
    "    f = open(path, 'rb')\n",
    "    soup = BeautifulSoup(f.read(), \"lxml\")\n",
    "    articles = soup.find_all(\"article\")\n",
    "\n",
    "    if len(articles) == 0:\n",
    "        articles = soup.find_all(id = 'op-content')\n",
    "\n",
    "    if len(articles) == 1: \n",
    "        ctr = ctr + 1\n",
    "        text = articles[0].text.translate ({ord(c): \"\" for c in chars_to_remove})\n",
    "        chosen.loc[index, 'text'] = text\n",
    "        chosen.loc[index, 'article'] = 1\n",
    "    else:\n",
    "        chosen.loc[index, 'text'] = 'no article'\n",
    "        chosen.loc[index, 'article'] = 0\n",
    "\n",
    "    f.close()\n",
    "\n",
    "    tm.sleep(.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1615\n"
     ]
    }
   ],
   "source": [
    "print(ctr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "chosen.drop('Unnamed: 0', axis=1).to_csv('chosen2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}