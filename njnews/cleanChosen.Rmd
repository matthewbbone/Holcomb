---
title: "Chosen Sample"
author: "Matthew Bone"
date: "3/15/2021"
output: 
  html_document:
    fig_height: 3
    fig_width: 5
  pdf_document:
    fig_height: 3
    fig_width: 5
  word_document:
    fig_height: 3
    fig_width: 5
---

This report takes a look the chosen sample from 12706-fulltext.txt. The sample includes all webpages that are from the domains below and from the dates below.

**Domains filtered on:**

'https://www.mypaperonline.com' (filtered by '/category/the-morristown-news' in a post-extraction step)

'https://www.newbrunswicktoday.com'

'https://www.newarkblack.com'

'http://www.mypaperonline.com' (filtered by '/category/the-morristown-news' in a post-extraction step)

'http://www.newbrunswicktoday.com'

'http://www.newarkblack.com'

'mypaperonline.com' (filtered by '/category/the-morristown-news' in a post-extraction step)

'newbrunswicktoday.com'

'newarkblack.com'

'www.mypaperonline.com' (filtered by '/category/the-morristown-news' in a post-extraction step)

'www.newbrunswicktoday.com'

'www.newarkblack.com'

**Dates filtered on:**

'20191013' (none found)

'20191104' (none found)

'20191001' 

'20191030' 

'20191024' 

'20190927' (none found)

'20191012' 

'20190930' 

'20191002' (none found)

'20191003' (none found)

'20191004'

**Important Findings: ** There don't seem to be any webpages from these three sources on 10/13, 11/04, 9/27, 10/02, or 10/03 in the dataset. Additionally, there are too few pages from www.mypaperonline.com/category/the-morristown-news for useful analysis.

```{r, setup, include=FALSE}
library(readr)
library(kableExtra)
library(tidyverse)
library(ggformula)
library(scales)
library(cld3)
library(stringr)
knitr::opts_chunk$set(
  tidy=FALSE,     # display code as typed
  size="small")   # slightly smaller font for code
```

```{r, include=FALSE}
chosen <- read_csv('chosensamp.csv')
```

```{r, include=FALSE}
n_entries <- chosen %>%
  nrow()

n_dom <- chosen %>%
  select(domain) %>%
  unique() %>%
  nrow()

n_url <- chosen %>%
  select(url) %>%
  unique() %>%
  nrow()
```

## Quick Numbers

Number of Articles: `r n_entries`

Number of Unique Domains: `r n_dom`

Number of Unique URLs: `r n_url`

```{r, include=FALSE}
samp <- chosen %>% 
  head()
```

```{r, echo=FALSE}
samp
```
## Counts by Domain

```{r, include=FALSE}
counts <- chosen %>%
  group_by(domain) %>%
  summarize(count = n()) %>%
  arrange(domain)  %>%
  kable(format = "markdown")
```

```{r, echo=FALSE}
counts 
```

## Counts by Date and Domain

```{r, include=FALSE}
counts <- chosen %>%
  group_by(date, domain) %>%
  summarize(count = n()) %>%
  arrange(date)  %>%
  kable(format = "markdown")
```

```{r, echo=FALSE}
counts 
```


## Date Distribution

```{r, echo=FALSE}
chosen %>%
  gf_bar(~date)
```

# Filtered by Domains but not Dates

I additionally extracted all data from the given websites (filtered on domain list above) to see if there's a significant amount of data to be used on other dates. It nearly doubles the count of the web pages in the sample but there's still very little data from www.mypaperonline.com/category/the-morristown-news. 

```{r, include=FALSE}
all_dates <- read_csv('all_dates.csv')
```

```{r, include=FALSE}
n_entries <- all_dates %>%
  nrow()

n_dom <- all_dates %>%
  select(domain) %>%
  unique() %>%
  nrow()

n_url <- all_dates %>%
  select(url) %>%
  unique() %>%
  nrow()
```

## Quick Numbers

Number of Articles: `r n_entries`

Number of Unique Domains: `r n_dom`

Number of Unique URLs: `r n_url`

```{r, include=FALSE}
samp <- all_dates %>% 
  head()
```

```{r, echo=FALSE}
samp
```

## Counts by Domain

```{r, include=FALSE}
counts <- all_dates %>%
  group_by(domain) %>%
  summarize(count = n()) %>%
  arrange(domain)  %>%
  kable(format = "markdown")
```

```{r, echo=FALSE}
counts 
```

## Counts by Date and Domain

```{r, include=FALSE}
counts <- all_dates %>%
  group_by(date, domain) %>%
  summarize(count = n()) %>%
  arrange(date) %>%
  kable(format = "markdown")
```

```{r, echo=FALSE}
counts 
```

## Date Distribution

```{r, echo=FALSE}
all_dates %>%
  gf_bar(~date)
```

