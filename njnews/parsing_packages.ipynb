{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.9 64-bit ('holcombEnv': conda)"
  },
  "interpreter": {
   "hash": "044a0165a1a561273ca818f13158b4edea630051cf64f3770cf4dedc4e818f34"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from newspaper import Article\n",
    "import pandas as pd\n",
    "import requests\n",
    "import numpy as np\n",
    "from datetime import datetime as dt\n",
    "import time as tm\n",
    "import re\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "      Unnamed: 0        date                 domain  \\\n",
       "0              0  2019-10-08    www.newarkblack.com   \n",
       "1              1  2019-10-08    www.newarkblack.com   \n",
       "2              2  2019-10-08    www.newarkblack.com   \n",
       "3              3  2019-10-08    www.newarkblack.com   \n",
       "4              4  2019-10-08    www.newarkblack.com   \n",
       "...          ...         ...                    ...   \n",
       "2374        2374  2019-10-01  www.mypaperonline.com   \n",
       "2376        2376  2019-10-01  www.mypaperonline.com   \n",
       "2393        2393  2019-10-12  www.mypaperonline.com   \n",
       "2399        2399  2019-11-02  www.mypaperonline.com   \n",
       "2405        2405  2019-11-02  www.mypaperonline.com   \n",
       "\n",
       "                                                    url  \\\n",
       "0                            http://www.newarkblack.com   \n",
       "1                            http://www.newarkblack.com   \n",
       "2     http://www.newarkblack.com/?s={search_term_str...   \n",
       "3                  http://www.newarkblack.com/advertise   \n",
       "4              http://www.newarkblack.com/category/news   \n",
       "...                                                 ...   \n",
       "2374        https://www.mypaperonline.com/category/food   \n",
       "2376   https://www.mypaperonline.com/category/nightlife   \n",
       "2393  https://www.mypaperonline.com/category/local-n...   \n",
       "2399  https://www.mypaperonline.com/category/local-n...   \n",
       "2405  https://www.mypaperonline.com/category/local-n...   \n",
       "\n",
       "                                                   text  cleaned  \\\n",
       "0     NewarkBlack.com - Newark African American News...        0   \n",
       "1                                                    �\b        0   \n",
       "2     You searched for {search_term_string} - Newark...        0   \n",
       "3     Advertise - NewarkBlack.com CLOSE Home Adverti...        0   \n",
       "4     News Archives - NewarkBlack.com CLOSE Home Adv...        0   \n",
       "...                                                 ...      ...   \n",
       "2374  Food Archive  My Paper Online Home Disclaimer ...        0   \n",
       "2376  Nightlife Archive  My Paper Online Home Discla...        0   \n",
       "2393  Local News Archive  My Paper Online Home Discl...        0   \n",
       "2399  Local News Archive  My Paper Online Home Discl...        0   \n",
       "2405  Local News Archive  My Paper Online Home Discl...        0   \n",
       "\n",
       "                                       title          published  \\\n",
       "0                                        NaN  September 25 2019   \n",
       "1                                        NaN  September 25 2019   \n",
       "2     {search_term_string} -  search results     October 7 2019   \n",
       "3                                        NaN     October 7 2019   \n",
       "4                                       News     October 3 2019   \n",
       "...                                      ...                ...   \n",
       "2374                                     NaN                NaN   \n",
       "2376                                     NaN   December 22 2015   \n",
       "2393                                     NaN                NaN   \n",
       "2399                                     NaN                NaN   \n",
       "2405                                     NaN                NaN   \n",
       "\n",
       "                           author  \n",
       "0                             NaN  \n",
       "1                             NaN  \n",
       "2                             NaN  \n",
       "3                             NaN  \n",
       "4     WA Public Relations Company  \n",
       "...                           ...  \n",
       "2374               new_view_media  \n",
       "2376               new_view_media  \n",
       "2393               new_view_media  \n",
       "2399               new_view_media  \n",
       "2405               new_view_media  \n",
       "\n",
       "[1063 rows x 9 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unnamed: 0</th>\n      <th>date</th>\n      <th>domain</th>\n      <th>url</th>\n      <th>text</th>\n      <th>cleaned</th>\n      <th>title</th>\n      <th>published</th>\n      <th>author</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>2019-10-08</td>\n      <td>www.newarkblack.com</td>\n      <td>http://www.newarkblack.com</td>\n      <td>NewarkBlack.com - Newark African American News...</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>September 25 2019</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>2019-10-08</td>\n      <td>www.newarkblack.com</td>\n      <td>http://www.newarkblack.com</td>\n      <td>�\b</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>September 25 2019</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>2019-10-08</td>\n      <td>www.newarkblack.com</td>\n      <td>http://www.newarkblack.com/?s={search_term_str...</td>\n      <td>You searched for {search_term_string} - Newark...</td>\n      <td>0</td>\n      <td>{search_term_string} -  search results</td>\n      <td>October 7 2019</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>2019-10-08</td>\n      <td>www.newarkblack.com</td>\n      <td>http://www.newarkblack.com/advertise</td>\n      <td>Advertise - NewarkBlack.com CLOSE Home Adverti...</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>October 7 2019</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>2019-10-08</td>\n      <td>www.newarkblack.com</td>\n      <td>http://www.newarkblack.com/category/news</td>\n      <td>News Archives - NewarkBlack.com CLOSE Home Adv...</td>\n      <td>0</td>\n      <td>News</td>\n      <td>October 3 2019</td>\n      <td>WA Public Relations Company</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>2374</th>\n      <td>2374</td>\n      <td>2019-10-01</td>\n      <td>www.mypaperonline.com</td>\n      <td>https://www.mypaperonline.com/category/food</td>\n      <td>Food Archive  My Paper Online Home Disclaimer ...</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>new_view_media</td>\n    </tr>\n    <tr>\n      <th>2376</th>\n      <td>2376</td>\n      <td>2019-10-01</td>\n      <td>www.mypaperonline.com</td>\n      <td>https://www.mypaperonline.com/category/nightlife</td>\n      <td>Nightlife Archive  My Paper Online Home Discla...</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>December 22 2015</td>\n      <td>new_view_media</td>\n    </tr>\n    <tr>\n      <th>2393</th>\n      <td>2393</td>\n      <td>2019-10-12</td>\n      <td>www.mypaperonline.com</td>\n      <td>https://www.mypaperonline.com/category/local-n...</td>\n      <td>Local News Archive  My Paper Online Home Discl...</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>new_view_media</td>\n    </tr>\n    <tr>\n      <th>2399</th>\n      <td>2399</td>\n      <td>2019-11-02</td>\n      <td>www.mypaperonline.com</td>\n      <td>https://www.mypaperonline.com/category/local-n...</td>\n      <td>Local News Archive  My Paper Online Home Discl...</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>new_view_media</td>\n    </tr>\n    <tr>\n      <th>2405</th>\n      <td>2405</td>\n      <td>2019-11-02</td>\n      <td>www.mypaperonline.com</td>\n      <td>https://www.mypaperonline.com/category/local-n...</td>\n      <td>Local News Archive  My Paper Online Home Discl...</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>new_view_media</td>\n    </tr>\n  </tbody>\n</table>\n<p>1063 rows × 9 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "cleaned = pd.read_csv('cleaned1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['Charlie Kratovil', 'About The Writer', 'More Writer', 'November', 'On', 'In', '.Molongui-Author-Box .Molongui-Author-Box-Tabs Nav.Molongui-Author-Box-Tabs-Top Background-Color', '.Molongui-Author-Box .Molongui-Author-Box-Tabs Nav Label Background-Color', '.Molongui-Author-Box .Molongui-Author-Box-Tabs Input Id', \"'Mab-Tab-Profile-'\"]\n2019-06-12 00:00:00\nUnder Pressure, Rutgers Releases RUPD and NBPD Dispatch Logs\n\n\nNEW BRUNSWICK, NJ—Rutgers University released 1,272 pages of police records after being served with an amended complaint in a public records lawsuit filed by this reporter.\n\nThe Computer Aided Dispatch (CAD) logs list each and every call that the city's two major police departments responded to between February 1 and May 31, plus other instances where officers were dispatched.\n\nAfter eighteen weeks of stalling, Rutgers released the records about 30 hours after being served with the amended complaint.\n\nThen, the following day, Rutgers released an additional 299 pages consisting of the logs from January 2019, 132 days after New Brunswick Today had first requested them.\n\nOn February 20, the university claimed it was \"attempting to export the data you have requested,\" and \"trying to determine whether we can produce the requested records,\" and began repeatedly granting themselves extensions to produce them over the next several months.\n\nEventually, a lawsuit was filed by this reporter, and this matter was added to the case on June 6, five days before Rutgers finally began to provide some of the information requested.\n\nThe litigation is still active in Middlesex County Superior Court, filed under docket #MID-L-4039-19. This reporter is represented by Walter Luers in the matter.\n\nRutgers took over the responsibility of maintaining these logs for New Brunswick's police department beginning on December 10, 2018.\n\nUPDATE (12:33pm): This article was updated to reflect the production of the January 2019 records.\n\n********************WE NEED YOUR HELP********************\n\nDo you see something in the records below that you think belongs in the news? Contact us at 732-993-9697.\n\nYou can also research incidents further by filing OPRA requests for reports or other records related to any calls of interest using the \"Dispatch Number\" and the website OPRAmachine.com.\n"
     ]
    }
   ],
   "source": [
    "article = Article('https://wayback.archive-it.org/12706/20200113151841/https://newbrunswicktoday.com/2019/06/12/under-pressure-rutgers-releases-rupd-and-nbpd-dispatch-logs/')\n",
    "#article.html = str(f.read(), 'utf-8')\n",
    "f.close()\n",
    "article.download()\n",
    "article.parse()\n",
    "print(article.authors)\n",
    "print(article.publish_date)\n",
    "print(article.title)\n",
    "print('\\n')\n",
    "print(article.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Timestamp('2019-10-08 00:00:00')"
      ]
     },
     "metadata": {},
     "execution_count": 37
    }
   ],
   "source": [
    "chosen.date[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# this creates and sets all the new variables to None\n",
    "chosen['cleaned'] = None\n",
    "chosen['title'] = None\n",
    "chosen['published'] = None\n",
    "chosen['author'] = None\n",
    "chosen['date'] = pd.to_datetime(chosen['date'])\n",
    "\n",
    "# this function constructs the url for an archived webpage according to https://wayback.archive-it.org formatting\n",
    "def get_html(collection_id, data):\n",
    "\n",
    "    # the base url for the archive\n",
    "    ai_url = \"https://wayback.archive-it.org\"\n",
    "\n",
    "    # this loop iterates through all the urls, searches for the archived webpage\n",
    "    # then looks for any hyperlinks on the page that are associated with a date in the list\n",
    "    for index, row in data.iterrows():\n",
    "\n",
    "        # this is the url of an archived webpage's home \n",
    "        # (e.g. https://wayback.archive-it.org/12706/*/http://mypaperonline.com)\n",
    "        home = ai_url + '/' + str(collection_id) + '/*/' + row.url\n",
    "\n",
    "        # this block gets all the date hyperlinks\n",
    "        page = requests.get(home)\n",
    "        soup = BeautifulSoup(page.content, \"lxml\")\n",
    "        alist = soup.find_all(\"a\")\n",
    "        alist = [a for a in alist if 'onclick' in a.attrs]\n",
    "        dates = [dt.strptime(a.text, '%b %d, %Y') for a in alist]\n",
    "\n",
    "        # this loops through all the date hyperlinks and if they're in\n",
    "        # the date_list it saves them locally\n",
    "        for a in alist:\n",
    "    \n",
    "            if dt.strptime(a.text, '%b %d, %Y') == row.date:\n",
    "                \n",
    "                article = Article('https:' + a.attrs['href'])\n",
    "\n",
    "                try:\n",
    "                    data.loc[index, 'title'] = article.title\n",
    "                    data.loc[index, 'author'] = article.authors[0]\n",
    "                    data.loc[index, 'published'] = article.published_date\n",
    "                    data.loc[index, 'text'] = article.text\n",
    "                    data.loc[index, 'cleaned'] = True\n",
    "                except:\n",
    "                    data.loc[index, 'cleaned'] = False\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "completed in  879.2199304103851  seconds\n"
     ]
    }
   ],
   "source": [
    "start = tm.time()\n",
    "new_parse = get_html(12706, chosen)\n",
    "span = tm.time() - start\n",
    "print('completed in ', span, ' seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_parse.to_csv('test_parsing.csv')"
   ]
  }
 ]
}