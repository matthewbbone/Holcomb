{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.9 64-bit ('holcombEnv': conda)"
  },
  "interpreter": {
   "hash": "044a0165a1a561273ca818f13158b4edea630051cf64f3770cf4dedc4e818f34"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from newspaper import Article\n",
    "import pandas as pd\n",
    "import requests\n",
    "import numpy as np\n",
    "from datetime import datetime as dt\n",
    "import time as tm\n",
    "import re\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "chosen = pd.read_csv('final_chosen.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['Charlie Kratovil', 'About The Writer', 'More Writer', 'November', 'On', 'In', '.Molongui-Author-Box .Molongui-Author-Box-Tabs Nav.Molongui-Author-Box-Tabs-Top Background-Color', '.Molongui-Author-Box .Molongui-Author-Box-Tabs Nav Label Background-Color', '.Molongui-Author-Box .Molongui-Author-Box-Tabs Input Id', \"'Mab-Tab-Profile-'\"]\n2019-06-12 00:00:00\nUnder Pressure, Rutgers Releases RUPD and NBPD Dispatch Logs\n"
     ]
    }
   ],
   "source": [
    "path  = 'chosenSamp/20191024_queen-of-harlem-michelle-smalls-set-to-receive-the-platinum-power-award-at-unlock-your-dreams-conference-expo-and-awards-gala.html'\n",
    "f = open(path, 'rb')\n",
    "article = Article('https://wayback.archive-it.org/12706/20200113151841/https://newbrunswicktoday.com/2019/06/12/under-pressure-rutgers-releases-rupd-and-nbpd-dispatch-logs/')\n",
    "#article.html = str(f.read(), 'utf-8')\n",
    "f.close()\n",
    "article.download()\n",
    "article.parse()\n",
    "print(article.authors)\n",
    "print(article.publish_date)\n",
    "print(article.title)\n",
    "#print(article.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Timestamp('2019-10-08 00:00:00')"
      ]
     },
     "metadata": {},
     "execution_count": 37
    }
   ],
   "source": [
    "chosen.date[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# this creates and sets all the new variables to None\n",
    "chosen['cleaned'] = None\n",
    "chosen['title'] = None\n",
    "chosen['published'] = None\n",
    "chosen['author'] = None\n",
    "chosen['date'] = pd.to_datetime(chosen['date'])\n",
    "\n",
    "# this function constructs the url for an archived webpage according to https://wayback.archive-it.org formatting\n",
    "def get_html(collection_id, data):\n",
    "\n",
    "    # the base url for the archive\n",
    "    ai_url = \"https://wayback.archive-it.org\"\n",
    "\n",
    "    # this loop iterates through all the urls, searches for the archived webpage\n",
    "    # then looks for any hyperlinks on the page that are associated with a date in the list\n",
    "    for index, row in data.iterrows():\n",
    "\n",
    "        # this is the url of an archived webpage's home \n",
    "        # (e.g. https://wayback.archive-it.org/12706/*/http://mypaperonline.com)\n",
    "        home = ai_url + '/' + str(collection_id) + '/*/' + row.url\n",
    "\n",
    "        # this block gets all the date hyperlinks\n",
    "        page = requests.get(home)\n",
    "        soup = BeautifulSoup(page.content, \"lxml\")\n",
    "        alist = soup.find_all(\"a\")\n",
    "        alist = [a for a in alist if 'onclick' in a.attrs]\n",
    "        dates = [dt.strptime(a.text, '%b %d, %Y') for a in alist]\n",
    "\n",
    "        # this loops through all the date hyperlinks and if they're in\n",
    "        # the date_list it saves them locally\n",
    "        for a in alist:\n",
    "    \n",
    "            if dt.strptime(a.text, '%b %d, %Y') == row.date:\n",
    "                \n",
    "                article = Article('https:' + a.attrs['href'])\n",
    "\n",
    "                try:\n",
    "                    data.loc[index, 'title'] = article.title\n",
    "                    data.loc[index, 'author'] = article.authors[0]\n",
    "                    data.loc[index, 'published'] = article.published_date\n",
    "                    data.loc[index, 'text'] = article.text\n",
    "                    data.loc[index, 'cleaned'] = True\n",
    "                except:\n",
    "                    data.loc[index, 'cleaned'] = False\n",
    "\n",
    "    return (data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "completed in  879.2199304103851  seconds\n"
     ]
    }
   ],
   "source": [
    "start = tm.time()\n",
    "new_parse = get_html(12706, chosen)\n",
    "span = tm.time() - start\n",
    "print('completed in ', span, ' seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_parse.to_csv('test_parsing.csv')"
   ]
  }
 ]
}