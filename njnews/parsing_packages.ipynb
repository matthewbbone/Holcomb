{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.9 64-bit ('holcombEnv': conda)"
  },
  "interpreter": {
   "hash": "044a0165a1a561273ca818f13158b4edea630051cf64f3770cf4dedc4e818f34"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from newspaper import Article\n",
    "import pandas as pd\n",
    "import requests\n",
    "import numpy as np\n",
    "from datetime import datetime as dt\n",
    "import time as tm\n",
    "import re\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned = pd.read_csv('cleaned1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['Charlie Kratovil', 'About The Writer', 'More Writer', 'November', 'On', 'In', '.Molongui-Author-Box .Molongui-Author-Box-Tabs Nav.Molongui-Author-Box-Tabs-Top Background-Color', '.Molongui-Author-Box .Molongui-Author-Box-Tabs Nav Label Background-Color', '.Molongui-Author-Box .Molongui-Author-Box-Tabs Input Id', \"'Mab-Tab-Profile-'\"]\n2019-06-12 00:00:00\nUnder Pressure, Rutgers Releases RUPD and NBPD Dispatch Logs\n\n\nNEW BRUNSWICK, NJâ€”Rutgers University released 1,272 pages of police records after being served with an amended complaint in a public records lawsuit filed by this reporter.\n\nThe Computer Aided Dispatch (CAD) logs list each and every call that the city's two major police departments responded to between February 1 and May 31, plus other instances where officers were dispatched.\n\nAfter eighteen weeks of stalling, Rutgers released the records about 30 hours after being served with the amended complaint.\n\nThen, the following day, Rutgers released an additional 299 pages consisting of the logs from January 2019, 132 days after New Brunswick Today had first requested them.\n\nOn February 20, the university claimed it was \"attempting to export the data you have requested,\" and \"trying to determine whether we can produce the requested records,\" and began repeatedly granting themselves extensions to produce them over the next several months.\n\nEventually, a lawsuit was filed by this reporter, and this matter was added to the case on June 6, five days before Rutgers finally began to provide some of the information requested.\n\nThe litigation is still active in Middlesex County Superior Court, filed under docket #MID-L-4039-19. This reporter is represented by Walter Luers in the matter.\n\nRutgers took over the responsibility of maintaining these logs for New Brunswick's police department beginning on December 10, 2018.\n\nUPDATE (12:33pm): This article was updated to reflect the production of the January 2019 records.\n\n********************WE NEED YOUR HELP********************\n\nDo you see something in the records below that you think belongs in the news? Contact us at 732-993-9697.\n\nYou can also research incidents further by filing OPRA requests for reports or other records related to any calls of interest using the \"Dispatch Number\" and the website OPRAmachine.com.\n"
     ]
    }
   ],
   "source": [
    "article = Article('https://wayback.archive-it.org/12706/20200113151841/https://newbrunswicktoday.com/2019/06/12/under-pressure-rutgers-releases-rupd-and-nbpd-dispatch-logs/')\n",
    "#article.html = str(f.read(), 'utf-8')\n",
    "article.download()\n",
    "article.parse()\n",
    "print(article.authors)\n",
    "print(article.publish_date)\n",
    "print(article.title)\n",
    "print('\\n')\n",
    "print(article.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'2019-10-08'"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "chosen.date[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# this creates and sets all the new variables to None\n",
    "chosen['cleaned'] = None\n",
    "chosen['title'] = None\n",
    "chosen['published'] = None\n",
    "chosen['author'] = None\n",
    "chosen['date'] = pd.to_datetime(chosen['date'])\n",
    "\n",
    "# this function constructs the url for an archived webpage according to https://wayback.archive-it.org formatting\n",
    "def get_html(collection_id, data):\n",
    "    ctr=0\n",
    "\n",
    "    # the base url for the archive\n",
    "    ai_url = \"https://wayback.archive-it.org\"\n",
    "\n",
    "    # this loop iterates through all the urls, searches for the archived webpage\n",
    "    # then looks for any hyperlinks on the page that are associated with a date in the list\n",
    "    for index, row in data.iterrows():\n",
    "\n",
    "        ctr+=1\n",
    "        # if ctr > 10: break\n",
    "\n",
    "        # this is the url of an archived webpage's home \n",
    "        # (e.g. https://wayback.archive-it.org/12706/*/http://mypaperonline.com)\n",
    "        home = ai_url + '/' + str(collection_id) + '/*/' + row.url\n",
    "\n",
    "        # this block gets all the date hyperlinks\n",
    "        page = requests.get(home)\n",
    "        soup = BeautifulSoup(page.content, \"lxml\")\n",
    "        alist = soup.find_all(\"a\")\n",
    "        alist = [a for a in alist if 'onclick' in a.attrs]\n",
    "        dates = [dt.strptime(a.text, '%b %d, %Y') for a in alist]\n",
    "\n",
    "        # this loops through all the date hyperlinks and if they're in\n",
    "        # the date_list it saves them locally\n",
    "        for a in alist:\n",
    "    \n",
    "            if dt.strptime(a.text, '%b %d, %Y') == row.date:\n",
    "                \n",
    "                try:\n",
    "                    article = Article('https:' + a.attrs['href'])\n",
    "                    article.download()\n",
    "                    article.parse()\n",
    "                except:\n",
    "                    print(\"can't proces\")\n",
    "\n",
    "                try:\n",
    "                    data.loc[index, 'title'] = article.title\n",
    "                    if len(article.authors) > 0: \n",
    "                        data.loc[index, 'author'] = article.authors[0]\n",
    "                    data.loc[index, 'published'] = article.publish_date\n",
    "                    data.loc[index, 'text'] = article.text\n",
    "                    data.loc[index, 'cleaned'] = 1\n",
    "                except:\n",
    "                    data.loc[index, 'cleaned'] = 0\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "can't proces\n",
      "can't proces\n",
      "C:\\Users\\matth\\anaconda3\\envs\\holcombEnv\\lib\\site-packages\\PIL\\TiffImagePlugin.py:772: UserWarning: Possibly corrupt EXIF data.  Expecting to read 32 bytes but only got 26. Skipping tag 42034\n",
      "  \"Possibly corrupt EXIF data.  \"\n",
      "C:\\Users\\matth\\anaconda3\\envs\\holcombEnv\\lib\\site-packages\\PIL\\TiffImagePlugin.py:772: UserWarning: Possibly corrupt EXIF data.  Expecting to read 19 bytes but only got 0. Skipping tag 42036\n",
      "  \"Possibly corrupt EXIF data.  \"\n",
      "can't proces\n",
      "C:\\Users\\matth\\anaconda3\\envs\\holcombEnv\\lib\\site-packages\\PIL\\TiffImagePlugin.py:772: UserWarning: Possibly corrupt EXIF data.  Expecting to read 918 bytes but only got 314. Skipping tag 37500\n",
      "  \"Possibly corrupt EXIF data.  \"\n",
      "C:\\Users\\matth\\anaconda3\\envs\\holcombEnv\\lib\\site-packages\\PIL\\TiffImagePlugin.py:772: UserWarning: Possibly corrupt EXIF data.  Expecting to read 8 bytes but only got 0. Skipping tag 41988\n",
      "  \"Possibly corrupt EXIF data.  \"\n",
      "C:\\Users\\matth\\anaconda3\\envs\\holcombEnv\\lib\\site-packages\\PIL\\TiffImagePlugin.py:772: UserWarning: Possibly corrupt EXIF data.  Expecting to read 32 bytes but only got 0. Skipping tag 42034\n",
      "  \"Possibly corrupt EXIF data.  \"\n",
      "C:\\Users\\matth\\anaconda3\\envs\\holcombEnv\\lib\\site-packages\\PIL\\TiffImagePlugin.py:772: UserWarning: Possibly corrupt EXIF data.  Expecting to read 6 bytes but only got 0. Skipping tag 42035\n",
      "  \"Possibly corrupt EXIF data.  \"\n",
      "C:\\Users\\matth\\anaconda3\\envs\\holcombEnv\\lib\\site-packages\\PIL\\TiffImagePlugin.py:772: UserWarning: Possibly corrupt EXIF data.  Expecting to read 35 bytes but only got 0. Skipping tag 42036\n",
      "  \"Possibly corrupt EXIF data.  \"\n",
      "can't proces\n",
      "can't proces\n",
      "can't proces\n",
      "can't proces\n",
      "can't proces\n",
      "can't proces\n",
      "completed in  2420.5372471809387  seconds\n"
     ]
    }
   ],
   "source": [
    "start = tm.time()\n",
    "new_parse = get_html(12706, chosen)\n",
    "span = tm.time() - start\n",
    "print('completed in ', span, ' seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_parse.to_csv('test_parsing.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}