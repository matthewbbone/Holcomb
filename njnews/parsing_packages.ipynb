{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.9 64-bit ('holcombEnv': conda)"
  },
  "interpreter": {
   "hash": "044a0165a1a561273ca818f13158b4edea630051cf64f3770cf4dedc4e818f34"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "from newspaper import Article\r\n",
    "import pandas as pd\r\n",
    "import requests\r\n",
    "import numpy as np\r\n",
    "from datetime import datetime as dt\r\n",
    "import time as tm\r\n",
    "import re\r\n",
    "from bs4 import BeautifulSoup\r\n",
    "import multiprocessing as mp"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "chosen = pd.read_csv('final_chosen.csv')\r\n",
    "folder= 'chosenSamp'"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "def overwrite(path):\r\n",
    "\r\n",
    "    print('async hit')\r\n",
    "\r\n",
    "    f = open(path, 'rb')\r\n",
    "    article = Article('')\r\n",
    "    article.download(f.read())\r\n",
    "    article.parse()\r\n",
    "\r\n",
    "    ret = []\r\n",
    "\r\n",
    "    try:\r\n",
    "        ret.append(article.title)\r\n",
    "        ret.append(article.publish_date)\r\n",
    "        ret.append(article.text)\r\n",
    "        if len(article.authors) > 0: \r\n",
    "            ret.append(article.authors[0])\r\n",
    "    except:\r\n",
    "        print('couldnt parse: ', path)\r\n",
    "\r\n",
    "    return ret\r\n",
    "\r\n",
    "results = []\r\n",
    "paths = []\r\n",
    "\r\n",
    "def collect_result(result):\r\n",
    "    global results\r\n",
    "    results.append(result)\r\n",
    "\r\n",
    "pool = mp.Pool(mp.cpu_count())\r\n",
    "\r\n",
    "for index, row in chosen.iterrows():\r\n",
    "\r\n",
    "    path = folder + '/' + ''.join(re.findall('\\d+', row.date)) + '_' + ''.join(re.findall('(\\d+|[a-zA-Z]+|-|\\.)', row.url)) + '.html'\r\n",
    "    paths.append(path)\r\n",
    "\r\n",
    "pool.map_async(overwrite, paths, callback=collect_result)\r\n",
    "pool.close()\r\n",
    "pool.join()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "display(results)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'2019-10-08'"
      ]
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "\r\n",
    "# this creates and sets all the new variables to None\r\n",
    "chosen['cleaned'] = None\r\n",
    "chosen['title'] = None\r\n",
    "chosen['published'] = None\r\n",
    "chosen['author'] = None\r\n",
    "\r\n",
    "def parse(collection_id, data, folder):\r\n",
    "    ctr=0\r\n",
    "\r\n",
    "    # the base url for the archive\r\n",
    "    ai_url = \"https://wayback.archive-it.org\"\r\n",
    "\r\n",
    "    # this loop iterates through all the urls, searches for the archived webpage\r\n",
    "    # then looks for any hyperlinks on the page that are associated with a date in the list\r\n",
    "    for index, row in data.iterrows():\r\n",
    "\r\n",
    "        ctr+=1\r\n",
    "        # if ctr > 10: break\r\n",
    "\r\n",
    "        # this is the url of an archived webpage's home \r\n",
    "        # (e.g. https://wayback.archive-it.org/12706/*/http://mypaperonline.com)\r\n",
    "        path = folder + '/' + ''.join(re.findall('\\d+', row.date)) + '_' + ''.join(re.findall('(\\d+|[a-zA-Z]+|-|\\.)', row.url)) + '.html'\r\n",
    "            \r\n",
    "        f = open(path, 'rb')\r\n",
    "        article = Article('')\r\n",
    "        article.download(f.read())\r\n",
    "        article.parse()\r\n",
    "\r\n",
    "        try:\r\n",
    "            data.loc[index, 'title'] = article.title\r\n",
    "            if len(article.authors) > 0: \r\n",
    "                data.loc[index, 'author'] = article.authors[0]\r\n",
    "            data.loc[index, 'published'] = article.publish_date\r\n",
    "            data.loc[index, 'text'] = article.text\r\n",
    "            data.loc[index, 'cleaned'] = 1\r\n",
    "        except:\r\n",
    "            data.loc[index, 'cleaned'] = 0\r\n",
    "\r\n",
    "    return data"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "start = tm.time()\r\n",
    "new_parse = parse(12706, chosen, 'chosenSamp')\r\n",
    "span = tm.time() - start\r\n",
    "print('completed in ', span, ' seconds')"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "ArticleException",
     "evalue": "You must `download()` an article first!",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mArticleException\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-02ca91837855>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mstart\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mnew_parse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m12706\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchosen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'chosenSamp'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mspan\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mstart\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'completed in '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mspan\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m' seconds'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-13-f0cafc827d82>\u001b[0m in \u001b[0;36mparse\u001b[1;34m(collection_id, data, folder)\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[0marticle\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mArticle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[0marticle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m         \u001b[0marticle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\holcombEnv\\lib\\site-packages\\newspaper\\article.py\u001b[0m in \u001b[0;36mparse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    189\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    190\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 191\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mthrow_if_not_downloaded_verbose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    192\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    193\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_parser\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfromstring\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhtml\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\holcombEnv\\lib\\site-packages\\newspaper\\article.py\u001b[0m in \u001b[0;36mthrow_if_not_downloaded_verbose\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    527\u001b[0m         \"\"\"\n\u001b[0;32m    528\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdownload_state\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mArticleDownloadState\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mNOT_STARTED\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 529\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mArticleException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'You must `download()` an article first!'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    530\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdownload_state\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mArticleDownloadState\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFAILED_RESPONSE\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    531\u001b[0m             raise ArticleException('Article `download()` failed with %s on URL %s' %\n",
      "\u001b[1;31mArticleException\u001b[0m: You must `download()` an article first!"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "new_parse.to_csv('new_test_parsing.csv')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# this creates and sets all the new variables to None\r\n",
    "chosen['cleaned'] = None\r\n",
    "chosen['title'] = None\r\n",
    "chosen['published'] = None\r\n",
    "chosen['author'] = None\r\n",
    "chosen['date'] = pd.to_datetime(chosen['date'])\r\n",
    "\r\n",
    "# this function constructs the url for an archived webpage according to https://wayback.archive-it.org formatting\r\n",
    "def old_parse(collection_id, data):\r\n",
    "    ctr=0\r\n",
    "\r\n",
    "    # the base url for the archive\r\n",
    "    ai_url = \"https://wayback.archive-it.org\"\r\n",
    "\r\n",
    "    # this loop iterates through all the urls, searches for the archived webpage\r\n",
    "    # then looks for any hyperlinks on the page that are associated with a date in the list\r\n",
    "    for index, row in data.iterrows():\r\n",
    "\r\n",
    "        ctr+=1\r\n",
    "        # if ctr > 10: break\r\n",
    "\r\n",
    "        # this is the url of an archived webpage's home \r\n",
    "        # (e.g. https://wayback.archive-it.org/12706/*/http://mypaperonline.com)\r\n",
    "        home = ai_url + '/' + str(collection_id) + '/*/' + row.url\r\n",
    "\r\n",
    "        # this block gets all the date hyperlinks\r\n",
    "        page = requests.get(home)\r\n",
    "        soup = BeautifulSoup(page.content, \"lxml\")\r\n",
    "        alist = soup.find_all(\"a\")\r\n",
    "        alist = [a for a in alist if 'onclick' in a.attrs]\r\n",
    "        dates = [dt.strptime(a.text, '%b %d, %Y') for a in alist]\r\n",
    "\r\n",
    "        # this loops through all the date hyperlinks and if they're in\r\n",
    "        # the date_list it saves them locally\r\n",
    "        for a in alist:\r\n",
    "    \r\n",
    "            if dt.strptime(a.text, '%b %d, %Y') == row.date:\r\n",
    "                \r\n",
    "                try:\r\n",
    "                    article = Article('https:' + a.attrs['href'])\r\n",
    "                    article.download()\r\n",
    "                    article.parse()\r\n",
    "                except:\r\n",
    "                    print(\"can't proces\")\r\n",
    "\r\n",
    "                try:\r\n",
    "                    data.loc[index, 'title'] = article.title\r\n",
    "                    if len(article.authors) > 0: \r\n",
    "                        data.loc[index, 'author'] = article.authors[0]\r\n",
    "                    data.loc[index, 'published'] = article.publish_date\r\n",
    "                    data.loc[index, 'text'] = article.text\r\n",
    "                    data.loc[index, 'cleaned'] = 1\r\n",
    "                except:\r\n",
    "                    data.loc[index, 'cleaned'] = 0\r\n",
    "\r\n",
    "    return data"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "start = tm.time()\r\n",
    "old_parse = old_parse(12706, chosen)\r\n",
    "span = tm.time() - start\r\n",
    "print('completed in ', span, ' seconds')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "old_parse.to_csv('test_parsing.csv')"
   ],
   "outputs": [],
   "metadata": {}
  }
 ]
}